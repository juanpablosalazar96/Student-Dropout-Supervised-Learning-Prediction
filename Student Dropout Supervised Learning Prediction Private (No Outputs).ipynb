{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7VaG1fIXoXq"
      },
      "source": [
        "# Applying supervised learning to predict student dropout\n",
        "\n",
        "\n",
        "In this project, we will examine student data and use supervised learning techniques to predict whether a student will drop out. In the education sector, retaining students is vital for the institution's financial stability and for students’ academic success and personal development. A high dropout rate can lead to significant revenue loss, diminished institutional reputation, and lower overall student satisfaction.\n",
        "\n",
        "We will work with the data in three distinct stages:\n",
        "\n",
        "1.  Applicant and course information\n",
        "2.  Student and engagement data\n",
        "3.  Academic performance data\n",
        "\n",
        "These stages reflect Study Group’s real-world data journey and how student information has progressed and become available. Additionally, this approach enables, through data exploration, to support Study Group in better understanding and identifying key metrics to monitor. This approach will also assist in determining at which stage of the student journey interventions would be most effective.\n",
        "\n",
        "## Business context\n",
        "Study Group specialises in providing educational services and resources to students and professionals across various fields. The company's primary focus is on enhancing learning experiences through a range of services, including online courses, tutoring, and educational consulting. By leveraging cutting-edge technology and a team of experienced educators, Study Group aims to bridge the gap between traditional learning methods and the evolving needs of today's learners.\n",
        "\n",
        "Study Group serves its university partners by establishing strategic partnerships to enhance the universities’ global reach and diversity. It supports the universities in their efforts to attract international students, thereby enriching the cultural and academic landscape of their campuses. It works closely with university faculty and staff to ensure that the universities are prepared and equipped to welcome and support a growing international student body. Its partnership with universities also offers international students a seamless transition into their chosen academic environment.\n",
        "\n",
        "Study Group runs several International Study Centres across the UK and Dublin in partnership with universities with the aim of preparing a pipeline of talented international students from diverse backgrounds for degree study. These centres help international students adapt to the academic, cultural, and social aspects of studying abroad. This is achieved by improving conversational and subject-specific language skills and academic readiness before students progress to a full degree programme at university.\n",
        "\n",
        "Through its comprehensive suite of services, it supports learners and universities at every stage of their educational journey, from high school to postgraduate studies. Its approach is tailored to meet the unique needs of each learner, offering personalised learning paths and flexible scheduling options to accommodate various learning styles and commitments.\n",
        "\n",
        "Study Group's services are designed to be accessible and affordable, making quality education a reality for many individuals. By focusing on the integration of technology and personalised learning, the company aims to empower learners to achieve their full potential and succeed in their academic and professional pursuits. Study Group is at the forefront of transforming how people learn and grow through its dedication to innovation and excellence.\n",
        "\n",
        "Study Groups provided 3 data sets.\n",
        "\n",
        "\n",
        "## Objective\n",
        "By the end of this mini-project, will have developed the skills and knowledge to apply advanced machine learning techniques to create a predictive model for student dropout. This project will involve comprehensive data exploration, preprocessing, and feature engineering to ensure high-quality input for the models. We will employ and compare multiple predictive algorithms, such as XGBoost, and a neural network-based model, to determine the most effective model for predicting student dropout.\n",
        "\n",
        "In the Notebook, we will:\n",
        "- explore the data sets, taking a phased approach\n",
        "- preprocess the data and conduct feature engineering\n",
        "- predict the dropout rate using XGBoost, and a neural network-based model.\n",
        "\n",
        "We will also provide a report summarising the results of our findings and recommendations.\n",
        "\n",
        "## Assessment criteria\n",
        "By completing this project, we’ll be able to provide evidence that can:\n",
        "\n",
        "- develop accurate predictions across diverse organisational scenarios by building and testing advanced ML models\n",
        "- inform data-driven decision-making with advanced machine learning algorithms and models\n",
        "- propose and present effective solutions to organisational problems using data preprocessing, model selection, and insightful analysis techniques.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJmJJ-pe_D5T"
      },
      "source": [
        "# Stage 1 data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRuNd4cIbHpd"
      },
      "source": [
        "I. Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9u2r8g69lh3"
      },
      "outputs": [],
      "source": [
        "# File URL\n",
        "file_url = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVh3xEFOAjpv"
      },
      "source": [
        "**Stage 1: Pre-processing instructions**\n",
        "- Remove any columns not useful in the analysis (LearnerCode).\n",
        "- Remove columns with high cardinality (use >200 unique values, as a guideline for this data set).\n",
        "- Remove columns with > 50% data missing.\n",
        "- Perform ordinal encoding for ordinal data.\n",
        "- Perform one-hot encoding for all other categorical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZnyK-e4mtMw"
      },
      "outputs": [],
      "source": [
        "# Start coding from here with Stage 1 dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv(file_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1rE5lM5T7Z6",
        "outputId": "6b0d107f-7775-44e6-833a-c038c571c68b"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "print(df.head())\n",
        "df.info()\n",
        "# Count the number of missing values in each column\n",
        "missing_counts = df.isnull().sum()\n",
        "print(missing_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TBCTLxibDX9"
      },
      "source": [
        "\n",
        "Remove any columns not useful in the analysis (LearnerCode).\n",
        "\n",
        "Remove columns with high cardinality (use >200 unique values, as a guideline for this data set).\n",
        "\n",
        "Remove columns with > 50% data missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcpBfxUL5kKJ",
        "outputId": "9f8b5d2f-eada-4480-a60e-29d3a3d3a74f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from datetime import datetime\n",
        "\n",
        "df[\"DateofBirth\"] = pd.to_datetime(df[\"DateofBirth\"], dayfirst=True)\n",
        "df[\"Age\"] = datetime.today().year - df[\"DateofBirth\"].dt.year\n",
        "df[\"Age\"] = df[\"Age\"].astype(int)  # Convert to integer\n",
        "df.drop(columns=[\"DateofBirth\"], inplace=True)\n",
        "\n",
        "\n",
        "###  Remove Unnecessary Columns ###\n",
        "df.drop(columns=[\"LearnerCode\"], inplace=True)\n",
        "\n",
        "### Remove High-Cardinality Columns ###\n",
        "high_cardinality_cols = [col for col in df.select_dtypes(include=[\"object\"]).columns\n",
        "                         if df[col].nunique() > 200]\n",
        "df.drop(columns=high_cardinality_cols, inplace=True)\n",
        "\n",
        "### Remove Columns with > 50% Missing Data ###\n",
        "missing_threshold = 0.5 * len(df)  # 50% of total rows around 12529.5 rows\n",
        "df.dropna(thresh=missing_threshold, axis=1, inplace=True)\n",
        "\n",
        "print(df.shape)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpOIGP2Ccmpp"
      },
      "source": [
        "Five columns have been erased during pre-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUcQpQKBdXuK"
      },
      "source": [
        "Convert the target variable from string to binary encoding.\n",
        "\n",
        "Perform ordinal encoding for\n",
        "ordinal data.\n",
        "\n",
        "Perform one-hot encoding for all other categorical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w4PIFza4r_W",
        "outputId": "de23f5a5-df16-4757-9bbc-d41d0e0bc92c"
      },
      "outputs": [],
      "source": [
        "#### 'CompletedCourse' is the target#### change to integer values\n",
        "df[\"CompletedCourse\"] = df[\"CompletedCourse\"].map({\"Yes\": 1, \"No\": 0})\n",
        "#### check unique values for Course level\n",
        "print(df[\"CourseLevel\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyeAQ7S7F1Cf"
      },
      "outputs": [],
      "source": [
        "### convert IsFirstIntake to integer\n",
        "df[\"IsFirstIntake\"] = df[\"IsFirstIntake\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXtm_UAfBfFP"
      },
      "outputs": [],
      "source": [
        "###  Transform Course level to ordinal data and encode it ###\n",
        "ordinal_features = [\"CourseLevel\"]\n",
        "ordinal_mapping = {\"Foundation\": 1, \"International Year One\": 2,\n",
        "                   \"International Year Two\": 3, \"Pre-Masters\": 4}\n",
        "df[\"CourseLevel\"] = df[\"CourseLevel\"].map(ordinal_mapping)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hipA3rUD2R8"
      },
      "outputs": [],
      "source": [
        "###  Convert Gender to 0 (Male) and 1 (Female) ###\n",
        "df[\"Gender\"] = df[\"Gender\"].map({\"Male\": 0, \"Female\": 1})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7a-PmRpDAc-",
        "outputId": "023d9e62-80f8-4ffa-ef88-df52799b784b"
      },
      "outputs": [],
      "source": [
        "df.info()\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DSJvi4nCjPl",
        "outputId": "bfde1d13-ccf6-4bd1-b376-eb21852d4646"
      },
      "outputs": [],
      "source": [
        "### One-Hot Encoding for Remaining Categorical Features ###\n",
        "categorical_columns = df.select_dtypes(include=[\"object\"]).columns\n",
        "df = pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "# Display the transformed dataset shape\n",
        "print(f\"New shape: {df.shape}\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJC2SY_MIpMI"
      },
      "source": [
        "Check for the target variable histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "bvjEp6wrIoak",
        "outputId": "2046c40e-ac02-4748-87d9-855ba9d6fd5a"
      },
      "outputs": [],
      "source": [
        "# Plot the histogram\n",
        "plt.hist(df['CompletedCourse'])\n",
        "plt.xlabel('Completed Course')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Completed Course')\n",
        "plt.show()\n",
        "\n",
        "## Count completed course values\n",
        "print(df['CompletedCourse'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1AdnDb7J3dT"
      },
      "source": [
        "The data is clearly imbalanced with over 5.5 more individuals that completed their respective course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctWxpBmrKR4k"
      },
      "source": [
        "In a first instance we will try the xgboost algorithm without performing feature reduction and check how th model performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss4czKyUW82a"
      },
      "source": [
        "Split the data for xgboost and neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSBGjYFiW48L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop(columns=[\"CompletedCourse\"])  # Features\n",
        "y = df[\"CompletedCourse\"]  # Target (Binary: 1 = Completed, 0 = Dropped Out)\n",
        "X = X.astype(int)  # Converts all boolean columns to 0/1 integers so that the neural network will be able to handle the data set\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q__DTAKmwNZ"
      },
      "source": [
        "Initialise XGBOOST model with default parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "Z-35HkamfFdK",
        "outputId": "36ed1885-f12e-446d-b399-5f77c3803ca2"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
        "seed = 42\n",
        "# Initialize XGBoost with default parameters\n",
        "xg_model = xgb.XGBClassifier(random_state=seed)  # Add seed for reproducibility\n",
        "\n",
        "# Train the model\n",
        "xg_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = xg_model.predict(X_test)\n",
        "# Make predictions\n",
        "y_pred = xg_model.predict(X_test)\n",
        "y_pred_proba = xg_model.predict_proba(X_test)[:, 1]  # Get probability for ROC-AUC\n",
        "\n",
        "# Compute Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "# Model evaluation\n",
        "print(\"XGBoost Model Accuracy:\", accuracy_score(y_test, predictions))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Feature Importance Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "feature_importances = xg_model.feature_importances_\n",
        "sorted_idx_1 = feature_importances.argsort()[-20:]  # Top 20 most important features\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(X.columns[sorted_idx_1], feature_importances[sorted_idx_1], color=\"royalblue\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.title(\"Top 20 Most Important Features (XGBoost)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEphAnQQuPHa",
        "outputId": "13616663-02ed-4176-9e4d-ae2a5b9d0423"
      },
      "outputs": [],
      "source": [
        "print(f\"Train Accuracy: {xg_model.score(X_train, y_train):.4f}\")\n",
        "print(f\"Test Accuracy: {xg_model.score(X_test, y_test):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYETRJsch1qm"
      },
      "source": [
        "These are the default parameters of xgboost that were used in the previuous cell. n_estimators: 100\n",
        "max_depth: 6\n",
        "learning_rate: 0.1\n",
        "reg_alpha: 0\n",
        "reg_lambda: 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9RjxSDIwIUu"
      },
      "source": [
        "In the next cell we will implement XGboost with 200 estimators and a learning rate of 0.05."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "NcBZrmLQXFLe",
        "outputId": "517b79a1-7d1f-4f23-c0cf-86bef5424406"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
        "# Initialize XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=200,      # Number of trees\n",
        "    learning_rate=0.05,    # Step size shrinkage\n",
        "    max_depth=6,           # Depth of each tree\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"logloss\"  # Since it's binary classification\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]  # Get probability for ROC-AUC\n",
        "\n",
        "# Compute Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Feature Importance Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "sorted_idx = feature_importances.argsort()[-20:]  # Top 20 most important features\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(X.columns[sorted_idx], feature_importances[sorted_idx], color=\"royalblue\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.title(\"Top 20 Most Important Features (XGBoost)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v0F8WkDlPNf",
        "outputId": "4cc006a4-b710-490b-d578-67a71021a336"
      },
      "outputs": [],
      "source": [
        "print(f\"Train Accuracy: {xgb_model.score(X_train, y_train):.4f}\")\n",
        "print(f\"Test Accuracy: {xgb_model.score(X_test, y_test):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEYtguoDwX1n"
      },
      "source": [
        "In the next cell we performed we performed a grid search where we tried models with three sets of estimators (100, 200, 300), three sets of learning rates (0.01, 0.05, and 0.1), and four sets of max depth leaves (3, 4, 5, 6)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tD81LYUaLnn",
        "outputId": "aa62ff5a-6f2e-46ed-c3ff-fe870235aec9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, precision_score, recall_score\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 200, 300],        # Number of trees\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1],     # Step size\n",
        "    \"max_depth\": [3, 4, 6, 8],                 # Tree depth\n",
        "\n",
        "}\n",
        "\n",
        "# Create Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb.XGBClassifier(\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\"\n",
        "    ),\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"roc_auc\",  # Optimize for AUC\n",
        "    cv=3,               # 3-fold cross-validation\n",
        "    verbose=2,\n",
        "    n_jobs=-1           # Use all CPU cores\n",
        ")\n",
        "\n",
        "# Fit on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Evaluate best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print performance metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Op9c7ww0AF"
      },
      "source": [
        "The grid search determined that the best model had 300 estimators, a 0.05 learning rate, and a max depth of 6 leaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "KaJ7nESNC4j0",
        "outputId": "8a31f3b3-b2d7-4cca-92f9-d14bf8f2f744"
      },
      "outputs": [],
      "source": [
        "# Feature Importance Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "feature_importances = best_model.feature_importances_\n",
        "sorted_idx_best = feature_importances.argsort()[-20:]  # Top 20 most important features\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(X.columns[sorted_idx_best], feature_importances[sorted_idx_best], color=\"royalblue\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.title(\"Top 20 Most Important Features (XGBoost)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kQ9n8qEpTNP"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "b_ohR2LUtd5f",
        "outputId": "1e468042-2a98-406c-ed59-51fc49927236"
      },
      "outputs": [],
      "source": [
        "shap_ex = shap.TreeExplainer(best_model)\n",
        "vals = shap_ex(X_test)\n",
        "shap.plots.beeswarm(vals)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rboTfJa3gSx3"
      },
      "source": [
        "The SHAP beeswarm plot provides a detailed view of feature importance by showing the magnitude and direction of each feature's impact on predictions. Additionally, it reveals feature interactions through the overlapping of SHAP values.\n",
        "\n",
        " The regular feature importance plot only displays absolute importance, while the SHAP plot highlights whether a feature increases or decreases the predicted outcome. This explains why some features, like course level and gender, appear lower in the regular plot. Furthermore, certain nationalities and center names seem highly important for predicting the target variable in both plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YldGrXBd0Hgk"
      },
      "source": [
        "Neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-KhcFeI0afA",
        "outputId": "7b7a2002-2f55-40a8-adc7-993150b5050c"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHcuTc9rtAuJ"
      },
      "source": [
        "In a first instance, we will create a straightforward neural network with one input layer, two hidden layers, and one output layer. Since the input has at least 395 features, this will be the number of neurons in the input layer. Then, we will reduce the number of neurons to 128 in the first hidden layer and reduce it by half in the next one. For the input and hidden layers, we will use the ReLU activation function, and for the output function, we will use the sigmoid function since it is a binary classification project. First, we will use the SGD optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXwl-Lju0FEp",
        "outputId": "c1676077-40c4-4a2d-9c1f-92f928bc21e9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(395,)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=SGD(learning_rate=0.01),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=20, batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "21So0NYHj8IA",
        "outputId": "d67fc21d-854b-4fe5-963d-9864d776711b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65R0Ie6-wJGL"
      },
      "source": [
        "The model already seems to have a good accuracy and  does not excessively overfit. We will try adding one hidden layer of 32 neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SV38JTJwIMo",
        "outputId": "6f68a63f-ec0a-453f-fe91-09e7546ea104"
      },
      "outputs": [],
      "source": [
        "# Build the neural network\n",
        "model_2 = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(395,)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model (Using SGD\n",
        "model_2.compile(\n",
        "    optimizer=SGD(learning_rate=0.01),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model_2\n",
        "history_2 = model_2.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=20, batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model_2.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model_2\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "QscNVFteyPR6",
        "outputId": "234c228c-5fcf-427c-8988-2de0535dc528"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming that the model has been trained and the history object has been created\n",
        "# history = model.fit(...)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_2.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_2.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_2.history['loss'], label='Training Loss')\n",
        "plt.plot(history_2.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBbS-QZ6yZV1"
      },
      "source": [
        "The model seems to perform slighltly better with one more layer with better accuracy and AUC score. So we will keep this structure with 3 hidden layers.\n",
        "\n",
        "In the next cell we will try tanh activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LwJgxLRySpX",
        "outputId": "6c4fdc51-bd9c-4b8b-8c1d-032b0f855b4c"
      },
      "outputs": [],
      "source": [
        "model_3 = Sequential([\n",
        "    Dense(256, activation='tanh', input_shape=(395,)),\n",
        "    Dense(128, activation='tanh'),\n",
        "    Dense(64, activation='tanh'),\n",
        "    Dense(32, activation='tanh'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model (Using SGD\n",
        "model_3.compile(\n",
        "    optimizer=SGD(learning_rate=0.01),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model_3\n",
        "history_3 = model_3.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=20, batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model_3.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model_3\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "v7y_iJ7r0ZNy",
        "outputId": "5ca1b75e-941b-4d30-8738-7bc195b8b204"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming that the model has been trained and the history object has been created\n",
        "# history = model.fit(...)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_3.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_3.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_3.history['loss'], label='Training Loss')\n",
        "plt.plot(history_3.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CIxxIxJ1p-T"
      },
      "source": [
        " Although using the tanh activation function in hidden layers led to a slight improvement in accuracy, the difference was not significant enough to justify switching from ReLU. Since ReLU is less prone to overfitting and is commonly used in deep networks, we will proceed with it to ensure a more stable and generalizable model. Moreover, the AUC score with ReLU was higher, indicating better overall classification performance across both classes.\n",
        "\n",
        " We will now proceed with a grid search to establish the best optimizer, batch size and number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRVKVhhy6qNf",
        "outputId": "fa5b0560-88f3-4e01-ba53-84e0ce55e8cd"
      },
      "outputs": [],
      "source": [
        "##### We had to install a previous version of scikit-learn to perform grid search on neural networks\n",
        "!pip install scikeras tensorflow keras scikit-learn\n",
        "!pip install scikit-learn==1.4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9SOpc1B3NOB",
        "outputId": "8b160601-dd1c-40ba-a585-a38ff558f1cf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Function to create the model\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "    # Define hyperparameters for grid search\n",
        "param_grid = {\n",
        "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
        "    'batch_size': [16, 32, 64],  # Try different batch sizes\n",
        "    'epochs': [10, 20, 30]  # Try different epoch values\n",
        "}\n",
        "\n",
        "# Wrap the model correctly\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)  # FIXED\n",
        "\n",
        "# Perform Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "grid_result = grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_result.best_params_)\n",
        "print(\"Best Accuracy:\", grid_result.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-E3hnfBLOFL"
      },
      "source": [
        "(The previous output printed the best accuracy, but it was supposed to show the best AUC score. Since it is computationally expensive, I did not run it again.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYIrgPVoVkRm"
      },
      "source": [
        "According to the previous grid search, keeping the neuronal architecture of model 2, the best performing model has 64 batch size, 10 epochs and the SGD optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_NY7sODO-JX",
        "outputId": "6d7e1c37-a1f1-4b37-973e-592e63591028"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Standardize features\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "model_4 = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(395,)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model (Using SGD\n",
        "model_4.compile(\n",
        "    optimizer=SGD(),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model_4\n",
        "history_4 = model_4.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=10, batch_size=64,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model_4.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model_4\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "n6461dzfoT4B",
        "outputId": "8ee73384-558a-4ab9-98d2-e90cd72f36b5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming that the model has been trained and the history object has been created\n",
        "# history = model.fit(...)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_4.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_4.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_4.history['loss'], label='Training Loss')\n",
        "plt.plot(history_4.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcAVsKjYLwPX"
      },
      "source": [
        "In the previous validation curves, we can see that the model starts to overfit around epoch 7. For Model 5, we introduced L1 and L2 regularization, as well as dropout, to mitigate overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2ttC9fz_dgo",
        "outputId": "a13400a8-79e7-4952-f5b4-ae96202f63e4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, callbacks\n",
        "\n",
        "# Define the model\n",
        "model_5 = keras.Sequential([\n",
        "    layers.Input(shape=(395,)),  # Input layer\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(128, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
        "    layers.Dropout(0.2),  # Dropout to prevent overfitting\n",
        "    layers.Dense(64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
        "    layers.Dropout(0.2),  # Dropout to prevent overfitting\n",
        "    layers.Dense(32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model_5\n",
        "model_5.compile(optimizer=SGD(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model_5\n",
        "history_5 = model_5.fit(X_train_scaled, y_train,\n",
        "                    validation_data=(X_test_scaled, y_test),\n",
        "                    epochs=10,\n",
        "                    batch_size=64,\n",
        "                    verbose=1)\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model_5.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "g_ffCWq0AK7C",
        "outputId": "dc889f78-0f5e-4944-e062-9a55f14ec46d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming that the model has been trained and the history object has been created\n",
        "# history = model.fit(...)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_5.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_5.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_5.history['loss'], label='Training Loss')\n",
        "plt.plot(history_5.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thYUbqeV1hT3"
      },
      "source": [
        "We can see in the previous loss curves that the regularizers mitigated overfitting although the auc score went down. Nonetheless, the performance is almost as good as before and the model is less prone to overfit.\n",
        "\n",
        "In the next cell, we will try a less agressive dropout to check if the model performs better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3DGp_wxO7nl",
        "outputId": "b5b1e39b-d14c-4732-889f-6c28b5a2767b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, callbacks\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, callbacks\n",
        "\n",
        "# Define the model\n",
        "model_6 = keras.Sequential([\n",
        "    layers.Input(shape=(395,)),  # Input layer\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(128, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0005, l2=0.0005)),\n",
        "    layers.Dropout(0.1),  # Dropout to prevent overfitting\n",
        "    layers.Dense(64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0005, l2=0.0005)),\n",
        "    layers.Dropout(0.1),  # Dropout to prevent overfitting\n",
        "    layers.Dense(32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0005, l2=0.0005)),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_6.compile(optimizer=SGD(),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=3,  # Stop if no improvement after 3 epochs\n",
        "    restore_best_weights=True,  # Restore best model weights\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model with early stopping\n",
        "history_6 = model_6.fit(X_train_scaled, y_train,\n",
        "                         validation_data=(X_test_scaled, y_test),\n",
        "                         epochs=10,\n",
        "                         batch_size=64,\n",
        "                         verbose=1,\n",
        "                         callbacks=[early_stopping])  # Add early stopping callback\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model_6.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcmir3mL4M1I"
      },
      "source": [
        "The model's performance is slightly worse than before, so we will keep Model 5 as the best NN model, as it is less prone to overfitting while maintaining optimal results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubV-Bnwb6NTd"
      },
      "source": [
        "Stage 1 Conclusion:\n",
        "XGBoost and NN models performed well on test and validation sets, achieving 89% accuracy. However, hyperparameter tuning did not yield significant improvements. Nevertheless, Model 5's regularization and dropout methods effectively prevented overfitting while maintaining similar performance, making it the best NN model.The tuned XGBoost outperformed the best neural network in AUC (88% vs. 75%), making it the best model overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S77lqNrM_Gck"
      },
      "source": [
        "# Stage 2 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dg28A4C-Noo"
      },
      "outputs": [],
      "source": [
        "# File URL\n",
        "file_url_2 = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nXlD5mqLzo0"
      },
      "outputs": [],
      "source": [
        "df_2 = pd.read_csv(file_url_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxuXX5FzMGgB",
        "outputId": "4be20dc2-da4f-434d-b02c-1cb541818f08"
      },
      "outputs": [],
      "source": [
        "print(df_2.shape)\n",
        "print(df_2.head())\n",
        "df_2.info()\n",
        "# Count the number of missing values in each column\n",
        "missing_counts = df_2.isnull().sum()\n",
        "print(missing_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--AkGtUFEOq9"
      },
      "source": [
        "**Stage 2: Pre-processing instructions**\n",
        "\n",
        "- Remove any columns not useful in the analysis (LearnerCode).\n",
        "- Remove columns with high cardinality (use >200 unique values, as a guideline for this data set).\n",
        "- Remove columns with >50% data missing.\n",
        "- Perform ordinal encoding for ordinal data.\n",
        "- Perform one-hot encoding for all other categorical data.\n",
        "- Choose how to engage with missing values, which can be done in one of two ways for this project:\n",
        "  *   Impute the rows with appropriate values.\n",
        "  *   Remove rows with missing values but ONLY in cases where rows with missing values are minimal: <2% of the overall data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCL_MVZqmqZS",
        "outputId": "732b1a13-fbb2-4caf-ef3d-7294a96a28fe"
      },
      "outputs": [],
      "source": [
        "# Start coding from here with Stage 2 dataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from datetime import datetime\n",
        "\n",
        "df_2[\"DateofBirth\"] = pd.to_datetime(df_2[\"DateofBirth\"], dayfirst=True)\n",
        "df_2[\"Age\"] = datetime.today().year - df_2[\"DateofBirth\"].dt.year\n",
        "df_2[\"Age\"] = df_2[\"Age\"].astype(int)  # Convert to integer\n",
        "df_2.drop(columns=[\"DateofBirth\"], inplace=True)\n",
        "\n",
        "\n",
        "###  Remove Unnecessary Columns ###\n",
        "df_2.drop(columns=[\"LearnerCode\"], inplace=True)\n",
        "\n",
        "### Remove High-Cardinality Columns ###\n",
        "high_cardinality_cols = [col for col in df_2.select_dtypes(include=[\"object\"]).columns\n",
        "                         if df_2[col].nunique() > 200]\n",
        "df_2.drop(columns=high_cardinality_cols, inplace=True)\n",
        "\n",
        "### Remove Columns with > 50% Missing Data ###\n",
        "missing_threshold = 0.5 * len(df_2)  # 50% of total rows around 12529.5 rows\n",
        "df_2.dropna(thresh=missing_threshold, axis=1, inplace=True)\n",
        "\n",
        "print(df_2.shape)\n",
        "print(df_2.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njqRJma4ODrM",
        "outputId": "9e51c62f-f1f0-409d-ab2a-dd112594f89f"
      },
      "outputs": [],
      "source": [
        "#### check unique values for Course level\n",
        "print(df_2[\"CourseLevel\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TPg8O_YNN8K",
        "outputId": "cc2ac7bd-6bba-4a1a-96d0-120cb7d434f1"
      },
      "outputs": [],
      "source": [
        "#### 'CompletedCourse' is the target#### change to integer values\n",
        "df_2[\"CompletedCourse\"] = df_2[\"CompletedCourse\"].map({\"Yes\": 1, \"No\": 0})\n",
        "### convert IsFirstIntake to integer\n",
        "df_2[\"IsFirstIntake\"] = df_2[\"IsFirstIntake\"].astype(int)\n",
        "\n",
        "###  Transform Course level to ordinal data and encode it ###\n",
        "ordinal_features = [\"CourseLevel\"]\n",
        "ordinal_mapping = {\"Foundation\": 1, \"International Year One\": 2,\n",
        "                   \"International Year Two\": 3, \"Pre-Masters\": 4}\n",
        "df_2[\"CourseLevel\"] = df_2[\"CourseLevel\"].map(ordinal_mapping)\n",
        "###  Convert Gender to 0 (Male) and 1 (Female) ###\n",
        "df_2[\"Gender\"] = df_2[\"Gender\"].map({\"Male\": 0, \"Female\": 1})\n",
        "\n",
        "print(df_2.shape)\n",
        "print(df_2.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyLrO0oFPLKL",
        "outputId": "d50e1dc7-59bf-44d0-f753-53215d6dc5c8"
      },
      "outputs": [],
      "source": [
        "### One-Hot Encoding for Remaining Categorical Features ### the new features are continuous\n",
        "categorical_columns = df_2.select_dtypes(include=[\"object\"]).columns\n",
        "df_2 = pd.get_dummies(df_2, columns=categorical_columns)\n",
        "\n",
        "# Display the transformed dataset shape\n",
        "print(f\"New shape: {df_2.shape}\")\n",
        "print(df_2.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQmGLUPUwp1Y"
      },
      "source": [
        "Handle missing values in the new features replacing them with the median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSWeBLRSwoyB"
      },
      "outputs": [],
      "source": [
        "##handle missing values in the two new features\n",
        "df_2['AuthorisedAbsenceCount'] = df_2['AuthorisedAbsenceCount'].fillna(df_2['AuthorisedAbsenceCount'].median())\n",
        "df_2['UnauthorisedAbsenceCount'] = df_2['UnauthorisedAbsenceCount'].fillna(df_2['UnauthorisedAbsenceCount'].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q37LTP0qxpRq",
        "outputId": "8598c714-6f56-453a-a32c-36bdd93e2c28"
      },
      "outputs": [],
      "source": [
        "print(df_2.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxsxWV3Ok-wk"
      },
      "source": [
        "Check if the data is still imbalanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "Wq4dZUorlCFn",
        "outputId": "4dc3cc0e-cb99-4d58-ef94-4884b64b6fc2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot the histogram\n",
        "plt.hist(df_2['CompletedCourse'])\n",
        "plt.xlabel('Completed Course')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Completed Course')\n",
        "plt.show()\n",
        "\n",
        "## Count completed course values\n",
        "print(df_2['CompletedCourse'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrMEA-aIl1Ge"
      },
      "source": [
        "For this new dataset, we will use the simplest XGBoost and neural network models we previously employed, as they performed well on the previous dataset (default XGBoost and NN Model 2 from Stage 1). We will split the data into training, validation, and test sets. Since the target is imbalanced, we will prioritize the AUC score, as it provides a better evaluation of model performance across different threshold values and is less affected by class imbalance than accuracy. Then, we will perform a grid search for both XGBoost and neural network models to select the best model for each method. Once we have chosen the best neural network architecture, we will apply techniques such as dropout and regularization to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9SNKmOquJ0F"
      },
      "source": [
        "Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DxNuU0duJi3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "\n",
        "# Define features and target\n",
        "X = df_2.drop(columns=[\"CompletedCourse\"])  # Features\n",
        "y = df_2[\"CompletedCourse\"]  # Target (Binary: 1 = Completed, 0 = Dropped Out)\n",
        "X = X.astype(int)  # Converts all boolean columns to 0/1 integers so that the neural network will be able to handle the data set ### the new features count the absence so it is okay to convert them to integers\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4tmHiLNyNcX"
      },
      "source": [
        "Performing default xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "KxYZ9KZ--3ib",
        "outputId": "fe0279cf-c5f5-4e26-c43e-632f3fdb4359"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
        "seed = 42\n",
        "# Initialize XGBoost with default parameters\n",
        "xg_model = xgb.XGBClassifier(random_state=seed)  # Add seed for reproducibility\n",
        "\n",
        "# Train the model\n",
        "xg_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = xg_model.predict(X_test)\n",
        "# Make predictions\n",
        "y_pred = xg_model.predict(X_test)\n",
        "y_pred_proba = xg_model.predict_proba(X_test)[:, 1]  # Get probability for ROC-AUC\n",
        "\n",
        "# Compute Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "# Model evaluation\n",
        "print(\"XGBoost Model Accuracy:\", accuracy_score(y_test, predictions))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Feature Importance Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "feature_importances = xg_model.feature_importances_\n",
        "sorted_idx_1 = feature_importances.argsort()[-20:]  # Top 20 most important features\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(X.columns[sorted_idx_1], feature_importances[sorted_idx_1], color=\"royalblue\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.title(\"Top 20 Most Important Features (XGBoost)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "LqdReWXMo06c",
        "outputId": "385fef4d-83ea-429b-b60d-4df7bf0295dd"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "shap_ex = shap.TreeExplainer(xg_model)\n",
        "vals = shap_ex(X_test)\n",
        "shap.plots.beeswarm(vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW6FPKOe9S5-"
      },
      "source": [
        "The beeswarm feature importance plot revealed that the new features had the highest importance, explaining the improved model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "GPojEI82yNIY",
        "outputId": "5331bafd-fe38-4928-e722-54785cf94836"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
        "# Initialize XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=200,      # Number of trees\n",
        "    learning_rate=0.05,    # Step size shrinkage\n",
        "    max_depth=6,           # Depth of each tree\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"logloss\"  # Since it's binary classification\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]  # Get probability for ROC-AUC\n",
        "\n",
        "# Compute Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Feature Importance Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "sorted_idx = feature_importances.argsort()[-20:]  # Top 20 most important features\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(X.columns[sorted_idx], feature_importances[sorted_idx], color=\"royalblue\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.title(\"Top 20 Most Important Features (XGBoost)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVtDQTZPaZuN",
        "outputId": "501c4633-a551-4f38-f016-e62e748872bc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, precision_score, recall_score\n",
        "\n",
        "# Define hyperparameter grid (3 values per parameter)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 200, 300],        # Number of trees\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1],     # Step size\n",
        "    \"max_depth\": [3, 4, 6, 8],                 # Tree depth\n",
        "}\n",
        "\n",
        "# Create Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb.XGBClassifier(\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\"\n",
        "    ),\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"roc_auc\",  # Optimize for AUC\n",
        "    cv=3,               # 3-fold cross-validation\n",
        "    verbose=2,\n",
        "    n_jobs=-1           # Use all CPU cores\n",
        ")\n",
        "\n",
        "# Fit on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Evaluate best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print performance metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "xgyq9dSFlnWG",
        "outputId": "1677aed3-cd58-48a1-bafc-a6a1cbe10c3e"
      },
      "outputs": [],
      "source": [
        "# Feature Importance Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "feature_importances = best_model.feature_importances_\n",
        "sorted_idx_best = feature_importances.argsort()[-20:]  # Top 20 most important features\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(X.columns[sorted_idx_best], feature_importances[sorted_idx_best], color=\"royalblue\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.title(\"Top 20 Most Important Features (XGBoost)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "4nx8uJ-ml0DW",
        "outputId": "684d458b-5280-4045-9c67-7128063607e7"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "shap_ex = shap.TreeExplainer(best_model)\n",
        "vals = shap_ex(X_test)\n",
        "shap.plots.beeswarm(vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku5YIJGx9YFB"
      },
      "source": [
        "The beeswarm feature importance plot revealed that the new features had the highest importance, explaining the improved model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y3T9lwlqGLw",
        "outputId": "0e9d03c2-854b-43cb-e6ba-4b98b95e2153"
      },
      "outputs": [],
      "source": [
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XqhVmx1YKd_q",
        "outputId": "f8d1a79f-3ce5-40d1-83de-67bcbcf69efd"
      },
      "outputs": [],
      "source": [
        "!pip install scikeras tensorflow keras scikit-learn\n",
        "!pip install scikit-learn==1.4.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtcP4Hoo7yC"
      },
      "source": [
        "I had to use a previous version of sckit-learn to perform gridsearch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oewtSVUM0Qr",
        "outputId": "0823cab0-2f60-42d6-8bd4-675ad8a696f0"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn==1.4.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7JAX24qkIw2"
      },
      "source": [
        "Neural network for stage 2:\n",
        "First we will try the best model of the previous stage(model 2) regarding accuracy and AUC score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-Y4_n9fkVJj",
        "outputId": "66f4e81a-764e-4a14-e7c0-b28a201b6783"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(397,)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model (Using SGD\n",
        "model.compile(\n",
        "    optimizer=SGD(learning_rate=0.01),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=20, batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "E3iwJhLLmwP6",
        "outputId": "b449616b-7a2a-4ee2-e39b-56876f4d86d1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming that the model has been trained and the history object has been created\n",
        "# history = model.fit(...)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VfVQsC_J1zF",
        "outputId": "88d28320-7603-45f9-f473-08b2c5038a86"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Function to create the model\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "    # Define hyperparameters for grid search\n",
        "param_grid = {\n",
        "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
        "    'batch_size': [16, 32, 64],  # Try different batch sizes\n",
        "    'epochs': [10, 20, 30]  # Try different epoch values\n",
        "}\n",
        "\n",
        "# Wrap the model correctly\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)  # FIXED\n",
        "\n",
        "# Perform Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "grid_result = grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_result.best_params_)\n",
        "print(\"Best AUC score\", grid_result.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrJpQkZjvr0T",
        "outputId": "3682ddf1-5859-4d0c-93c1-781702a64567"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "# Build the neural network\n",
        "model_2 = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(397,)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model_2 (Using SGD\n",
        "model_2.compile(\n",
        "    optimizer=RMSprop(),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model_2\n",
        "history_2 = model_2.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=10, batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model_2.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model_2\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "1oLR29RFwtGu",
        "outputId": "1dcb2506-4d82-42c1-dc2f-68e9346e16a1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_2.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_2.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_2.history['loss'], label='Train Loss')\n",
        "plt.plot(history_2.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMHOpUuGkTdy"
      },
      "source": [
        "The model seems to be overftting since the loss validation curve. In the next model we will implement l1/l2 regularizers and dropout to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwccdHf30NHu",
        "outputId": "489ebed0-0063-4b3d-d331-4967b929be84"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, callbacks\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "# Define the model\n",
        "model_3 = keras.Sequential([\n",
        "    layers.Input(shape=(397,)),  # Input layer,\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(128, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
        "    layers.Dropout(0.2),  # Dropout to prevent overfitting\n",
        "    layers.Dense(64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model_3\n",
        "model_3.compile(optimizer=RMSprop(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Train the model_3\n",
        "history_3 = model_3.fit(X_train_scaled, y_train,\n",
        "                    validation_data=(X_test_scaled, y_test),\n",
        "                    epochs=10,\n",
        "                    batch_size=64)\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model_3.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Xk3VHlkI-iQQ",
        "outputId": "64dcdb3f-d549-4fd0-99a9-5ce86069fe74"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_3.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_3.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_3.history['loss'], label='Train Loss')\n",
        "plt.plot(history_3.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m43RjAOg9tGt"
      },
      "source": [
        "We can see in the previous figures that dropout and regularizers prevented overfitting and outputted smoother curves.\n",
        "\n",
        "Stage 2 conclusion:\n",
        "\n",
        "NN Model 3 introduced L1/L2 regularization and dropout, effectively reducing overfitting while maintaining performance.\n",
        "\n",
        "The improvements from hyperparameter tuning in XGBoost and NN models were not significant. However, the additional information provided by the two new features, \"UnauthorisedAbsenceCount\" and \"AuthorisedAbsenceCount,\" allowed the models to achieve over 90% accuracy. The beeswarm feature importance plot revealed that the new features had the highest importance, explaining the improved model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMe8QiJB_Kwc"
      },
      "source": [
        "# Stage 3 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0Rigjfl-e5q"
      },
      "outputs": [],
      "source": [
        "# File URL\n",
        "file_url_3 = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XskDovobNBRD"
      },
      "outputs": [],
      "source": [
        "# Start coding from here with Stage 1 dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "import xgboost as xgb\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELLcskkfMraw",
        "outputId": "c805bd5b-6a75-48b8-cd4e-4fc762961703"
      },
      "outputs": [],
      "source": [
        "df_3 = pd.read_csv(file_url_3)\n",
        "print(df_3.shape)\n",
        "print(df_3.head())\n",
        "df_3.info()\n",
        "# Count the number of missing values in each column\n",
        "missing_counts = df_3.isnull().sum()\n",
        "print(missing_counts)\n",
        "df_3.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMN7RgGfHIcn"
      },
      "source": [
        "**Stage 3: Pre-processing instructions**\n",
        "\n",
        "- Remove any columns not useful in the analysis (LearnerCode).\n",
        "- Remove columns with high cardinality (use >200 unique values, as a guideline for this data set).\n",
        "- Remove columns with >50% data missing.\n",
        "- Perform ordinal encoding for ordinal data.\n",
        "- Perform one-hot encoding for all other categorical data.\n",
        "- Choose how to engage with rows that have missing values, which can be done in one of two ways for this project:\n",
        "  *   Impute the rows with appropriate values.\n",
        "  *   Remove rows with missing values but ONLY in cases where rows with missing values are minimal: <2% of the overall data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx-Ioa4OmbYB",
        "outputId": "084bf749-cc05-4539-ce55-b038a6b2189c"
      },
      "outputs": [],
      "source": [
        "# Start coding from here with Stage 3 dataset\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from datetime import datetime\n",
        "\n",
        "df_3[\"DateofBirth\"] = pd.to_datetime(df_3[\"DateofBirth\"], dayfirst=True)\n",
        "df_3[\"Age\"] = datetime.today().year - df_3[\"DateofBirth\"].dt.year\n",
        "df_3[\"Age\"] = df_3[\"Age\"].astype(int)  # Convert to integer\n",
        "df_3.drop(columns=[\"DateofBirth\"], inplace=True)\n",
        "\n",
        "\n",
        "###  Remove Unnecessary Columns ###\n",
        "df_3.drop(columns=[\"LearnerCode\"], inplace=True)\n",
        "\n",
        "### Remove High-Cardinality Columns ###\n",
        "high_cardinality_cols = [col for col in df_3.select_dtypes(include=[\"object\"]).columns\n",
        "                         if df_3[col].nunique() > 200]\n",
        "df_3.drop(columns=high_cardinality_cols, inplace=True)\n",
        "\n",
        "### Remove Columns with > 50% Missing Data ###\n",
        "missing_threshold = 0.5 * len(df_3)  # 50% of total rows around 12529.5 rows\n",
        "df_3.dropna(thresh=missing_threshold, axis=1, inplace=True)\n",
        "\n",
        "print(df_3.shape)\n",
        "print(df_3.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTTdZwNnTmQq",
        "outputId": "cea5b7d4-5e25-4a5c-a64b-f9c2fac7e6a5"
      },
      "outputs": [],
      "source": [
        "#### 'CompletedCourse' is the target#### change to integer values\n",
        "df_3[\"CompletedCourse\"] = df_3[\"CompletedCourse\"].map({\"Yes\": 1, \"No\": 0})\n",
        "### One-Hot Encoding for Remaining Categorical Features ### the new features are continuous\n",
        "categorical_columns = df_3.select_dtypes(include=[\"object\"]).columns\n",
        "df_3 = pd.get_dummies(df_3, columns=categorical_columns)\n",
        "\n",
        "# Display the transformed dataset shape\n",
        "print(f\"New shape: {df_3.shape}\")\n",
        "print(df_3.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGZzYsDiUL1V"
      },
      "outputs": [],
      "source": [
        "##handle missing values in the five new features replacing them with the median\n",
        "df_3['AuthorisedAbsenceCount'] = df_3['AuthorisedAbsenceCount'].fillna(df_3['AuthorisedAbsenceCount'].median())\n",
        "df_3['UnauthorisedAbsenceCount'] = df_3['UnauthorisedAbsenceCount'].fillna(df_3['UnauthorisedAbsenceCount'].median())\n",
        "df_3['AssessedModules'] = df_3['AssessedModules'].fillna(df_3['AssessedModules'].median())\n",
        "df_3['PassedModules'] = df_3['PassedModules'].fillna(df_3['PassedModules'].median())\n",
        "df_3['FailedModules'] = df_3['FailedModules'].fillna(df_3['FailedModules'].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwB1ar_wVr5U",
        "outputId": "94371e72-3479-47af-eef4-e49f0e99935d"
      },
      "outputs": [],
      "source": [
        "print(df_3.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJgPQ4eLVqUM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "\n",
        "# Define features and target\n",
        "X = df_3.drop(columns=[\"CompletedCourse\"])\n",
        "y = df_3[\"CompletedCourse\"]  # Target (Binary: 1 = Completed, 0 = Dropped Out)\n",
        "X = X.astype(int)  # Converts all boolean columns to 0/1 integers so that the neural network will be able to handle the data set ### the new features are integers\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JQsCiIpZD5U"
      },
      "source": [
        "Performing simple xg boost before grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "9Z7Vf-JmZCOy",
        "outputId": "c58a994b-5c38-439a-f6c6-7cec6e89a572"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
        "seed = 42\n",
        "# Initialize XGBoost with default parameters\n",
        "xg_model = xgb.XGBClassifier(random_state=seed)  # Add seed for reproducibility\n",
        "\n",
        "# Train the model\n",
        "xg_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = xg_model.predict(X_test)\n",
        "# Make predictions\n",
        "y_pred = xg_model.predict(X_test)\n",
        "y_pred_proba = xg_model.predict_proba(X_test)[:, 1]  # Get probability for ROC-AUC\n",
        "\n",
        "# Compute Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "# Model evaluation\n",
        "print(\"XGBoost Model Accuracy:\", accuracy_score(y_test, predictions))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Feature Importance Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "feature_importances = xg_model.feature_importances_\n",
        "sorted_idx_1 = feature_importances.argsort()[-20:]  # Top 20 most important features\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(X.columns[sorted_idx_1], feature_importances[sorted_idx_1], color=\"royalblue\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.title(\"Top 20 Most Important Features (XGBoost)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "kZJHyhEpZxfl",
        "outputId": "7fa3fc1c-b85c-4a3d-fc86-f4ebef6b5e0b"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "shap_ex = shap.TreeExplainer(xg_model)\n",
        "vals = shap_ex(X_test)\n",
        "shap.plots.beeswarm(vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19cPIuR5aqYs",
        "outputId": "fed3a5b7-23a5-49bc-82f5-3627c1057dfb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, precision_score, recall_score\n",
        "\n",
        "# Define hyperparameter grid (at least 3 values per parameter)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 200, 300],        # Number of trees\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1],     # Step size\n",
        "    \"max_depth\": [3, 4, 6, 8],                 # Tree depth\n",
        "}\n",
        "\n",
        "# Create Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb.XGBClassifier(\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\"\n",
        "    ),\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"roc_auc\",  # Optimize for AUC\n",
        "    cv=3,               # 3-fold cross-validation\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Evaluate best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print performance metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "fL45IiKRclhL",
        "outputId": "a33d775b-3675-49ff-a256-fe1761e8f101"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "shap_ex = shap.TreeExplainer(best_model)\n",
        "vals = shap_ex(X_test)\n",
        "shap.plots.beeswarm(vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyJVOQlY_YGD"
      },
      "source": [
        "The beeswarm SHAP PLOT for stage 3 shows that the newly introduced features have the highest importance and explain why the models peroformances are way higher in stage 3. The regular feature importance barplot only displays absolute importance, while the SHAP plot highlights whether a feature increases or decreases the predicted outcome. This explains why the new features appear lower in the regular plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vsKY2s4s6z8"
      },
      "source": [
        "Neural Networks Stage 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMMehnmWtecw",
        "outputId": "5b58f565-0195-4606-a38c-bf753b8cc2c1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(404,)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model (Using SGD\n",
        "model.compile(\n",
        "    optimizer=SGD(learning_rate=0.01),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=20, batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "7ATPptDNvOA3",
        "outputId": "d957f3a3-1426-4ad7-9bf7-5f16d9726134"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming that the model has been trained and the history object has been created\n",
        "# history = model.fit(...)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0_UqoCxu9m8",
        "outputId": "f375dd0d-a51a-47e5-cfd7-c2b1049bc304"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Function to create the model\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "    # Define hyperparameters for grid search\n",
        "param_grid = {\n",
        "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
        "    'batch_size': [16, 32, 64],  # Try different batch sizes\n",
        "    'epochs': [10, 20]  # Try different epoch values\n",
        "}\n",
        "\n",
        "# Wrap the model correctly\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)  # FIXED\n",
        "\n",
        "# Perform Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "grid_result = grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_result.best_params_)\n",
        "print(\"Best AUC score\", grid_result.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRv6-g026djY",
        "outputId": "f5a1b84d-94e0-422e-b55c-b09b005bf6c6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Build the neural network\n",
        "model_2 = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(404,)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Compile the model (Using SGD\n",
        "model_2.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history_2 = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=10, batch_size=64,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "OXvXFvnjGdCK",
        "outputId": "2caf5fab-3ffe-4e17-8d76-ca9d81d10d36"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming that the model has been trained and the history object has been created\n",
        "# history = model.fit(...)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_2.history_['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_2.history_['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_2.history_['loss'], label='Training Loss')\n",
        "plt.plot(history_2.history_['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMq00TVdHI77",
        "outputId": "ba5a8c56-7c74-49fd-f199-be65b7e0b5da"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, callbacks\n",
        "from keras.optimizers import RMSprop, Adam# Define the model\n",
        "model_3 = keras.Sequential([\n",
        "    layers.Input(shape=(404,)),  # Input layer\n",
        "    layers.Dense(128, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
        "    layers.Dropout(0.2),  # Dropout to prevent overfitting\n",
        "    layers.Dense(64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model_3\n",
        "model_3.compile(optimizer=\"adam\",\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Train the model_3\n",
        "history_3 = model_3.fit(X_train_scaled, y_train,\n",
        "                    validation_data=(X_test_scaled, y_test),\n",
        "                    epochs=10,\n",
        "                    batch_size=64)\n",
        "# Make predictions\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "OG_Hg0T6Iiw1",
        "outputId": "f2a9897c-8874-4ffd-d58f-59ab16a97d27"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming that the model has been trained and the history object has been created\n",
        "# history = model.fit(...)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_3.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_3.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_3.history['loss'], label='Training Loss')\n",
        "plt.plot(history_3.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2xhWaVC-dZT"
      },
      "source": [
        "Once again, regularization and dropout methods have mitigated overfitting, resulting in smoother validation loss curves in NN Model 3 for Stage 3 while maintaining similar performance. Because of this, NN Model 3 for Stage 3 is expected to generalize better to new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDPJGmY8BXrP"
      },
      "source": [
        "Stage 3 Conclusion:\n",
        "\n",
        "Hyperparameter tuning had no significant impact on performance, but the inclusion of three new student performance features significantly improved results. XGBoost achieved 98% accuracy and an AUC of 99%, while the neural network models reached 96% accuracy and an AUC of 92%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWPCxpsAGZ__"
      },
      "source": [
        "Conclusion: Our results emphasize the importance of data quality in improving model performance. While hyperparameter tuning had no significant impact, adding relevant features significantly enhanced accuracy and AUC scores. L1 and L2 regularization effectively mitigated overfitting in neural network models, improving generalization. Additionally, XGBoost consistently achieved the best performance across all three stages. Future improvements could involve expanding the grid search, addressing class imbalance, or applying feature selection techniques to refine model efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GnYizObz65"
      },
      "source": [
        "# Declaration\n",
        "By submitting your project, you indicate that the work is your own and has been created with academic integrity. Refer to the **Cambridge plagiarism regulations**.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
